{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ecb846c",
   "metadata": {},
   "source": [
    "# Step 16: In-Memory Vector Store Cache\n",
    "\n",
    "This notebook demonstrates caching LLM responses using in-memory vector store with semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d2d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2365ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from collections.abc import Awaitable, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Annotated\n",
    "from uuid import uuid4\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.embedding_generator_base import EmbeddingGeneratorBase\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    AzureChatCompletion,\n",
    "    AzureTextEmbedding,\n",
    ")\n",
    "from semantic_kernel.connectors.in_memory import InMemoryStore\n",
    "from semantic_kernel.data.vector import (\n",
    "    VectorStoreField,\n",
    "    vectorstoremodel,\n",
    "    FieldTypes,\n",
    "    VectorSearchOptions,\n",
    "    VectorStore,\n",
    "    VectorStoreCollection,\n",
    ")\n",
    "from semantic_kernel.filters import (\n",
    "    FilterTypes,\n",
    "    FunctionInvocationContext,\n",
    "    PromptRenderContext,\n",
    ")\n",
    "from semantic_kernel.functions import FunctionResult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31c7c3b",
   "metadata": {},
   "source": [
    "## Define Cache Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a10d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorstoremodel\n",
    "@dataclass\n",
    "class CacheRecord:\n",
    "    prompt: Annotated[str, VectorStoreField(is_indexed=True)]\n",
    "    result: Annotated[str, VectorStoreField(is_full_text_indexed=True)]\n",
    "    prompt_embedding: Annotated[\n",
    "        list[float], VectorStoreField(field_type=FieldTypes.VECTOR, dimensions=1536)\n",
    "    ] = field(default_factory=list)\n",
    "    id: Annotated[str, VectorStoreField(field_type=FieldTypes.KEY)] = field(\n",
    "        default_factory=lambda: str(uuid4())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a57a1",
   "metadata": {},
   "source": [
    "## Define Prompt Cache Filter\n",
    "\n",
    "This filter intercepts prompt rendering and function invocation to implement semantic caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c40428",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"llm_responses\"\n",
    "RECORD_ID_KEY = \"cache_record_id\"\n",
    "\n",
    "\n",
    "class PromptCacheFilter:\n",
    "    \"\"\"A filter to cache the results of the prompt rendering and function invocation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_service: EmbeddingGeneratorBase,\n",
    "        vector_store: VectorStore,\n",
    "        collection_name: str = COLLECTION_NAME,\n",
    "        score_threshold: float = 0.2,\n",
    "    ):\n",
    "        self.embedding_service = embedding_service\n",
    "        self.vector_store = vector_store\n",
    "        self.collection: VectorStoreCollection[str, CacheRecord] = (\n",
    "            vector_store.get_collection(CacheRecord, collection_name=collection_name)\n",
    "        )\n",
    "        self.score_threshold = score_threshold\n",
    "\n",
    "    async def on_prompt_render(\n",
    "        self,\n",
    "        context: PromptRenderContext,\n",
    "        next: Callable[[PromptRenderContext], Awaitable[None]],\n",
    "    ):\n",
    "        \"\"\"Filter to cache the rendered prompt and the result of the function.\n",
    "\n",
    "        It uses the score threshold to determine if the result should be cached.\n",
    "        The direction of the comparison is based on the default distance metric for\n",
    "        the in memory vector store, which is cosine distance, so the closer to 0 the\n",
    "        closer the match.\n",
    "        \"\"\"\n",
    "        await next(context)\n",
    "        assert context.rendered_prompt  # nosec\n",
    "        prompt_embedding = await self.embedding_service.generate_raw_embeddings(\n",
    "            [context.rendered_prompt]\n",
    "        )\n",
    "        # Collection is created automatically on first use\n",
    "        results = await self.collection.search(\n",
    "            vector=prompt_embedding[0],\n",
    "            vector_property_name=\"prompt_embedding\",\n",
    "            top=1,\n",
    "        )\n",
    "        async for result in results.results:\n",
    "            if result.score < self.score_threshold:\n",
    "                context.function_result = FunctionResult(\n",
    "                    function=context.function.metadata,\n",
    "                    value=result.record.result,\n",
    "                    rendered_prompt=context.rendered_prompt,\n",
    "                    metadata={RECORD_ID_KEY: result.record.id},\n",
    "                )\n",
    "\n",
    "    async def on_function_invocation(\n",
    "        self,\n",
    "        context: FunctionInvocationContext,\n",
    "        next: Callable[[FunctionInvocationContext], Awaitable[None]],\n",
    "    ):\n",
    "        \"\"\"Filter to store the result in the cache if it is new.\"\"\"\n",
    "        await next(context)\n",
    "        result = context.result\n",
    "        if result and result.rendered_prompt and RECORD_ID_KEY not in result.metadata:\n",
    "            prompt_embedding = await self.embedding_service.generate_embeddings(\n",
    "                [result.rendered_prompt]\n",
    "            )\n",
    "            cache_record = CacheRecord(\n",
    "                prompt=result.rendered_prompt,\n",
    "                result=str(result),\n",
    "                prompt_embedding=prompt_embedding[0],\n",
    "            )\n",
    "            # Collection is created automatically on first use\n",
    "            await self.collection.upsert(cache_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4135e72",
   "metadata": {},
   "source": [
    "## Setup Kernel and Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56058822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize kernel and services\n",
    "kernel = Kernel()\n",
    "chat = AzureChatCompletion(service_id=\"default\")\n",
    "embedding = AzureTextEmbedding(service_id=\"embedder\")\n",
    "kernel.add_service(chat)\n",
    "kernel.add_service(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2958ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create in-memory vector store\n",
    "vector_store = InMemoryStore()\n",
    "print(\"Vector store initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622e8ea",
   "metadata": {},
   "source": [
    "## Register Cache Filter\n",
    "\n",
    "Create and register the cache filter with the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64196a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the cache filter and add it to the kernel\n",
    "cache = PromptCacheFilter(embedding_service=embedding, vector_store=vector_store)\n",
    "kernel.add_filter(FilterTypes.PROMPT_RENDERING, cache.on_prompt_render)\n",
    "kernel.add_filter(FilterTypes.FUNCTION_INVOCATION, cache.on_function_invocation)\n",
    "print(\"Cache filter registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab82782",
   "metadata": {},
   "source": [
    "## Test Semantic Caching\n",
    "\n",
    "Run queries to demonstrate semantic caching behavior. Similar queries should be retrieved from cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0a85de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "async def execute_async(title: str, prompt: str):\n",
    "    print(f\"{title}: {prompt}\")\n",
    "    start = time.time()\n",
    "    result = await kernel.invoke_prompt(prompt)\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"\\tElapsed Time: {elapsed:.3f}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2ea4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First query - will make an actual LLM call\n",
    "result1 = await execute_async(\"First run\", \"What's the tallest building in New York?\")\n",
    "print(f\"Result 1: {result1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aaf06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second query - different topic, will make another LLM call\n",
    "result2 = await execute_async(\"Second run\", \"How are you today?\")\n",
    "print(f\"Result 2: {result2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3c6ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third query - semantically similar to first, should retrieve from cache (faster!)\n",
    "result3 = await execute_async(\"Third run\", \"What is the highest building in New York City?\")\n",
    "print(f\"Result 3: {result3}\")\n",
    "print(\"\\nâš¡ Notice the significantly reduced time for the third query - it was retrieved from cache!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e039cc78",
   "metadata": {},
   "source": [
    "## Interactive Testing\n",
    "\n",
    "Use the loop below to test with your own queries and observe caching behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c87fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive loop for testing queries\n",
    "# Type 'exit' to quit\n",
    "while True:\n",
    "    user_input = input(\"User > \")\n",
    "    \n",
    "    if user_input == \"exit\":\n",
    "        break\n",
    "    \n",
    "    result = await execute_async(\"Test\", user_input)\n",
    "    print(f\"Result: {result}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
