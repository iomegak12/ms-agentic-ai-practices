{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eebc189",
   "metadata": {},
   "source": [
    "# Step 15: Prompt Rendering Filters\n",
    "\n",
    "This notebook demonstrates how to use filters to modify prompts before and after rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac2c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "from semantic_kernel.filters import PromptRenderContext, FilterTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7094f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system message\n",
    "system_message = \"\"\"\n",
    "You are a chat bot. Your name is RK Bot.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912abc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt rendering filter\n",
    "async def prompt_rendering_filter(context: PromptRenderContext, next):\n",
    "    print(\"Before Rendering...\")\n",
    "    await next(context)\n",
    "    print(\"After Rendering...\")\n",
    "    \n",
    "    # Modify the rendered prompt\n",
    "    context.rendered_prompt = f\"In case you found any city names, just say, I can answer about countries and not city like 'Just give me a country name and not city' {context.rendered_prompt or ''}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3469ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize kernel and add filter\n",
    "kernel = Kernel()\n",
    "service_id = \"chat-gpt\"\n",
    "kernel.add_service(AzureChatCompletion(service_id=service_id))\n",
    "kernel.add_filter(FilterTypes.PROMPT_RENDERING, prompt_rendering_filter)\n",
    "\n",
    "settings = kernel.get_prompt_execution_settings_from_service_id(service_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0839c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chat function\n",
    "chat_function = kernel.add_function(\n",
    "    plugin_name=\"ChatBot\",\n",
    "    function_name=\"Chat\",\n",
    "    prompt=\"{{$chat_history}}{{$user_input}}\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    prompt_execution_settings=settings,\n",
    ")\n",
    "\n",
    "chat_history = ChatHistory(system_message=system_message)\n",
    "chat_history.add_user_message(\"Hi there, who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e1cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive chat function\n",
    "async def chat(user_input: str):\n",
    "    answer = await kernel.invoke(\n",
    "        chat_function,\n",
    "        KernelArguments(\n",
    "            user_input=user_input,\n",
    "            chat_history=str(chat_history),\n",
    "        ),\n",
    "    )\n",
    "    chat_history.add_user_message(user_input)\n",
    "    chat_history.add_assistant_message(str(answer))\n",
    "    print(f\"RK Bot:> {answer}\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066a7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Test the filter\n",
    "await chat(\"where am i located?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d2a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive chat loop\n",
    "# Type 'exit' to quit\n",
    "while True:\n",
    "    user_input = input(\"User > \")\n",
    "    \n",
    "    if user_input == \"exit\":\n",
    "        break\n",
    "    \n",
    "    await chat(user_input)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
