{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d4c366",
   "metadata": {},
   "source": [
    "# Step 11: Semantic Memory and RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "This notebook demonstrates how to store facts in memory, search them semantically, and use them to improve LLM responses using RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdeedd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    AzureTextEmbedding,\n",
    "    AzureChatCompletion,\n",
    ")\n",
    "from semantic_kernel.memory import SemanticTextMemory, VolatileMemoryStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d3b6a0",
   "metadata": {},
   "source": [
    "## Initialize Kernel and Services\n",
    "\n",
    "We set up the Semantic Kernel with:\n",
    "1. **AzureTextEmbedding**: Converts text into embeddings (vector representations)\n",
    "2. **AzureChatCompletion**: Chat-based LLM for generating responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9889ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Kernel()\n",
    "\n",
    "embedding_service = AzureTextEmbedding(service_id=\"embedding\")\n",
    "chat_service = AzureChatCompletion(service_id=\"chat\")\n",
    "kernel.add_service(embedding_service)\n",
    "kernel.add_service(chat_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8057f8ed",
   "metadata": {},
   "source": [
    "## Understanding Embeddings\n",
    "\n",
    "Let's see how embeddings work with two similar sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b76c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS = [\n",
    "    \"A dog ran joyfully through the green field, chasing after butterflies in the warm afternoon sun.\",\n",
    "    \"A happy puppy sprinted across the grassy meadow, playfully pursuing insects under the bright sky.\",\n",
    "]\n",
    "\n",
    "# Generate embeddings for similar texts\n",
    "text_embedded = await embedding_service.generate_embeddings(TEXTS)\n",
    "print(\"üî¢ Embeddings generated for similar texts\")\n",
    "print(f\"Embedding dimensions: {len(text_embedded[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c847d9",
   "metadata": {},
   "source": [
    "## Set Up Semantic Memory\n",
    "\n",
    "We use `SemanticTextMemory` with `VolatileMemoryStore` (in-memory, temporary storage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57075120",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SemanticTextMemory(\n",
    "    storage=VolatileMemoryStore(), \n",
    "    embeddings_generator=embedding_service\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e8def7",
   "metadata": {},
   "source": [
    "## Save Facts to Memory\n",
    "\n",
    "Let's store some travel-related facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec03050",
   "metadata": {},
   "outputs": [],
   "source": [
    "await memory.save_information(\n",
    "    collection=\"travel_notes\",  \n",
    "    id=\"note1\", \n",
    "    text=\"User is currently in Barcelona.\", \n",
    ")\n",
    "await memory.save_information(\n",
    "    collection=\"travel_notes\",\n",
    "    id=\"note2\",\n",
    "    text=\"User enjoys modern art museums and seaside walks.\",\n",
    ")\n",
    "await memory.save_information(\n",
    "    collection=\"travel_notes\",\n",
    "    id=\"note3\",\n",
    "    text=\"Today is Saturday and the user is free in the afternoon.\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Facts saved to memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bc111a",
   "metadata": {},
   "source": [
    "## Semantic Search\n",
    "\n",
    "Now we'll search memory for relevant facts using semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe203f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What should I recommend for this afternoon?\"\n",
    "\n",
    "results = await memory.search(collection=\"travel_notes\", query=query, limit=3)\n",
    "\n",
    "print(f\"\\nüîç Semantic Query: {query}\")\n",
    "for r in results:\n",
    "    print(f\"‚úÖ Match: '{r.text}' (score: {r.relevance:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dc7eae",
   "metadata": {},
   "source": [
    "## RAG: Response WITH Memory Context\n",
    "\n",
    "Let's use the top match as context for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba1ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_info = results[0].text\n",
    "prompt_with_context = f\"Based on this context: '{context_info}', what can I suggest to do this afternoon?\"\n",
    "response_with_context = await kernel.invoke_prompt(prompt_with_context)\n",
    "\n",
    "print(\"\\n--- üß† LLM Response WITH Memory Context ---\")\n",
    "print(response_with_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2211615a",
   "metadata": {},
   "source": [
    "## Response WITHOUT Memory Context\n",
    "\n",
    "For comparison, let's ask the same question without context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac2ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_without_context = \"What can I suggest to do this afternoon?\"\n",
    "response_without_context = await kernel.invoke_prompt(prompt_without_context)\n",
    "\n",
    "print(\"\\n--- ‚ùì LLM Response WITHOUT Memory ---\")\n",
    "print(response_without_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e56bb",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've now seen Retrieval-Augmented Generation (RAG) in action:\n",
    "1. ‚úÖ Store facts in semantic memory\n",
    "2. üîç Search using semantic similarity\n",
    "3. üß† Use retrieved context to improve LLM responses\n",
    "\n",
    "This makes LLMs more reliable, accurate, and contextual!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
